\subsubsection{Registration Network}
The basis of the model fine-tuned in this paper is the SynthMorph model developed by Hoffmann et al. (2023)\cite{synthmorphModel}. There are several variants of SynthMorph, including one that generates a dense displacement field. In this paper however, we focus on the variants that produce rigid and affine transformations. Since we are concerned here with pairs of scans from the same patient, any non-rigid deformation is very likely to be incorrect.

The original Synthmorph is trained using purely synthetic images, generated from anatomical label maps. The synthetic images are varied significantly during training to cover a wide range of real-world scenarios, including different image contrasts and resolutions. This approach enables the base network to effectively handle various modalities, such as T1 and T2 MRI scans. The goal of our work is to extend this capability beyond just multi-modal compatibility to achieve robust cross-domain performance as well.

The actual network architecture is fully convolutional, utilising eight 3D convolutional layers, which output spatial feature maps separately for the moving and fixed images. The barycentres of these feature maps are then calculated, and the points are used to analytically align the two images with weighted-least-squares.

%Training on Synthetic Data: The model is trained using a novel approach where synthetic images are generated from anatomical label maps. These synthetic images are intentionally varied to cover a wide range of potential real-world scenarios, including different imaging contrasts and resolutions. This allows the model to learn registration in a way that is robust to changes in acquisition parameters, such as MRI contrast or noise.

%Anatomy-Aware Registration: Unlike traditional methods that may require preprocessing steps like skull stripping to focus on the relevant anatomy, SynthMorph incorporates anatomy-specific learning directly into the model. By optimizing the overlap of selected anatomical labels (such as white matter, gray matter, and cerebrospinal fluid), the model becomes adept at aligning the relevant structures while ignoring irrelevant image content.

%Affine Transform Prediction: The model predicts an affine transform that aligns the input image pair by fitting the transformation to match the anatomical features. This involves computing barycenters and weights for each predicted feature map, and then aligning these using a weighted least-squares approach.

%Implementation Details: The model architecture uses a convolutional neural network (CNN) to extract features from the images, with the output being a full affine transformation.


\subsubsection{Segmentation Network}
The SynthSeg network, by Billot et al. (2023)\cite{synthseg1}\cite{synthseg2} is also used in this paper. This network is used out of the box however, and is not fine-tuned. Instead, it is utilised to provide a less abstract, and more clinically relevant and interpretable metric for final evaluation of the fine-tuned registration models. The manner in which this is done is detailed in the Evaluation Criteria subsection

The SynthSeg model is a 3D UNet\cite{unet} type network, trained on synthetic data like the Synthmorph network. The synthetic data used to train this network is generated based on a corresponding synthetic label map, that is itself generated by a transformation of a label map from the training data. This approach allows the model to generalise well over different image types, including T1 scans as utilised in this paper.
